{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:10.425798Z",
     "start_time": "2024-12-08T08:46:10.418169Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:15.687997Z",
     "start_time": "2024-12-08T08:46:10.679934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)  # 6 labels for your task\n",
    "#load from file\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rubert-classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rubert-classifier\")\n",
    "\n",
    "# Заморозка всех слоев, кроме последнего\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Замораживаем все параметры\n",
    "\n",
    "# Размораживаем параметры последнего слоя (classifier)\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True  # Размораживаем параметры последнего слоя\n",
    "\n",
    "# Проверка\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")\n"
   ],
   "id": "6003fafc518089e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 08:46:14.255941: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-08 08:46:14.269999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-08 08:46:14.285088: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-08 08:46:14.290293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 08:46:14.302951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-08 08:46:15.245792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight: False\n",
      "bert.embeddings.position_embeddings.weight: False\n",
      "bert.embeddings.token_type_embeddings.weight: False\n",
      "bert.embeddings.LayerNorm.weight: False\n",
      "bert.embeddings.LayerNorm.bias: False\n",
      "bert.encoder.layer.0.attention.self.query.weight: False\n",
      "bert.encoder.layer.0.attention.self.query.bias: False\n",
      "bert.encoder.layer.0.attention.self.key.weight: False\n",
      "bert.encoder.layer.0.attention.self.key.bias: False\n",
      "bert.encoder.layer.0.attention.self.value.weight: False\n",
      "bert.encoder.layer.0.attention.self.value.bias: False\n",
      "bert.encoder.layer.0.attention.output.dense.weight: False\n",
      "bert.encoder.layer.0.attention.output.dense.bias: False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.0.intermediate.dense.weight: False\n",
      "bert.encoder.layer.0.intermediate.dense.bias: False\n",
      "bert.encoder.layer.0.output.dense.weight: False\n",
      "bert.encoder.layer.0.output.dense.bias: False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.1.attention.self.query.weight: False\n",
      "bert.encoder.layer.1.attention.self.query.bias: False\n",
      "bert.encoder.layer.1.attention.self.key.weight: False\n",
      "bert.encoder.layer.1.attention.self.key.bias: False\n",
      "bert.encoder.layer.1.attention.self.value.weight: False\n",
      "bert.encoder.layer.1.attention.self.value.bias: False\n",
      "bert.encoder.layer.1.attention.output.dense.weight: False\n",
      "bert.encoder.layer.1.attention.output.dense.bias: False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.1.intermediate.dense.weight: False\n",
      "bert.encoder.layer.1.intermediate.dense.bias: False\n",
      "bert.encoder.layer.1.output.dense.weight: False\n",
      "bert.encoder.layer.1.output.dense.bias: False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.2.attention.self.query.weight: False\n",
      "bert.encoder.layer.2.attention.self.query.bias: False\n",
      "bert.encoder.layer.2.attention.self.key.weight: False\n",
      "bert.encoder.layer.2.attention.self.key.bias: False\n",
      "bert.encoder.layer.2.attention.self.value.weight: False\n",
      "bert.encoder.layer.2.attention.self.value.bias: False\n",
      "bert.encoder.layer.2.attention.output.dense.weight: False\n",
      "bert.encoder.layer.2.attention.output.dense.bias: False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.2.intermediate.dense.weight: False\n",
      "bert.encoder.layer.2.intermediate.dense.bias: False\n",
      "bert.encoder.layer.2.output.dense.weight: False\n",
      "bert.encoder.layer.2.output.dense.bias: False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.3.attention.self.query.weight: False\n",
      "bert.encoder.layer.3.attention.self.query.bias: False\n",
      "bert.encoder.layer.3.attention.self.key.weight: False\n",
      "bert.encoder.layer.3.attention.self.key.bias: False\n",
      "bert.encoder.layer.3.attention.self.value.weight: False\n",
      "bert.encoder.layer.3.attention.self.value.bias: False\n",
      "bert.encoder.layer.3.attention.output.dense.weight: False\n",
      "bert.encoder.layer.3.attention.output.dense.bias: False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.3.intermediate.dense.weight: False\n",
      "bert.encoder.layer.3.intermediate.dense.bias: False\n",
      "bert.encoder.layer.3.output.dense.weight: False\n",
      "bert.encoder.layer.3.output.dense.bias: False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.4.attention.self.query.weight: False\n",
      "bert.encoder.layer.4.attention.self.query.bias: False\n",
      "bert.encoder.layer.4.attention.self.key.weight: False\n",
      "bert.encoder.layer.4.attention.self.key.bias: False\n",
      "bert.encoder.layer.4.attention.self.value.weight: False\n",
      "bert.encoder.layer.4.attention.self.value.bias: False\n",
      "bert.encoder.layer.4.attention.output.dense.weight: False\n",
      "bert.encoder.layer.4.attention.output.dense.bias: False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.4.intermediate.dense.weight: False\n",
      "bert.encoder.layer.4.intermediate.dense.bias: False\n",
      "bert.encoder.layer.4.output.dense.weight: False\n",
      "bert.encoder.layer.4.output.dense.bias: False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.5.attention.self.query.weight: False\n",
      "bert.encoder.layer.5.attention.self.query.bias: False\n",
      "bert.encoder.layer.5.attention.self.key.weight: False\n",
      "bert.encoder.layer.5.attention.self.key.bias: False\n",
      "bert.encoder.layer.5.attention.self.value.weight: False\n",
      "bert.encoder.layer.5.attention.self.value.bias: False\n",
      "bert.encoder.layer.5.attention.output.dense.weight: False\n",
      "bert.encoder.layer.5.attention.output.dense.bias: False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.5.intermediate.dense.weight: False\n",
      "bert.encoder.layer.5.intermediate.dense.bias: False\n",
      "bert.encoder.layer.5.output.dense.weight: False\n",
      "bert.encoder.layer.5.output.dense.bias: False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.6.attention.self.query.weight: False\n",
      "bert.encoder.layer.6.attention.self.query.bias: False\n",
      "bert.encoder.layer.6.attention.self.key.weight: False\n",
      "bert.encoder.layer.6.attention.self.key.bias: False\n",
      "bert.encoder.layer.6.attention.self.value.weight: False\n",
      "bert.encoder.layer.6.attention.self.value.bias: False\n",
      "bert.encoder.layer.6.attention.output.dense.weight: False\n",
      "bert.encoder.layer.6.attention.output.dense.bias: False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.6.intermediate.dense.weight: False\n",
      "bert.encoder.layer.6.intermediate.dense.bias: False\n",
      "bert.encoder.layer.6.output.dense.weight: False\n",
      "bert.encoder.layer.6.output.dense.bias: False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.7.attention.self.query.weight: False\n",
      "bert.encoder.layer.7.attention.self.query.bias: False\n",
      "bert.encoder.layer.7.attention.self.key.weight: False\n",
      "bert.encoder.layer.7.attention.self.key.bias: False\n",
      "bert.encoder.layer.7.attention.self.value.weight: False\n",
      "bert.encoder.layer.7.attention.self.value.bias: False\n",
      "bert.encoder.layer.7.attention.output.dense.weight: False\n",
      "bert.encoder.layer.7.attention.output.dense.bias: False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.7.intermediate.dense.weight: False\n",
      "bert.encoder.layer.7.intermediate.dense.bias: False\n",
      "bert.encoder.layer.7.output.dense.weight: False\n",
      "bert.encoder.layer.7.output.dense.bias: False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.8.attention.self.query.weight: False\n",
      "bert.encoder.layer.8.attention.self.query.bias: False\n",
      "bert.encoder.layer.8.attention.self.key.weight: False\n",
      "bert.encoder.layer.8.attention.self.key.bias: False\n",
      "bert.encoder.layer.8.attention.self.value.weight: False\n",
      "bert.encoder.layer.8.attention.self.value.bias: False\n",
      "bert.encoder.layer.8.attention.output.dense.weight: False\n",
      "bert.encoder.layer.8.attention.output.dense.bias: False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.8.intermediate.dense.weight: False\n",
      "bert.encoder.layer.8.intermediate.dense.bias: False\n",
      "bert.encoder.layer.8.output.dense.weight: False\n",
      "bert.encoder.layer.8.output.dense.bias: False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.9.attention.self.query.weight: False\n",
      "bert.encoder.layer.9.attention.self.query.bias: False\n",
      "bert.encoder.layer.9.attention.self.key.weight: False\n",
      "bert.encoder.layer.9.attention.self.key.bias: False\n",
      "bert.encoder.layer.9.attention.self.value.weight: False\n",
      "bert.encoder.layer.9.attention.self.value.bias: False\n",
      "bert.encoder.layer.9.attention.output.dense.weight: False\n",
      "bert.encoder.layer.9.attention.output.dense.bias: False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.9.intermediate.dense.weight: False\n",
      "bert.encoder.layer.9.intermediate.dense.bias: False\n",
      "bert.encoder.layer.9.output.dense.weight: False\n",
      "bert.encoder.layer.9.output.dense.bias: False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.10.attention.self.query.weight: False\n",
      "bert.encoder.layer.10.attention.self.query.bias: False\n",
      "bert.encoder.layer.10.attention.self.key.weight: False\n",
      "bert.encoder.layer.10.attention.self.key.bias: False\n",
      "bert.encoder.layer.10.attention.self.value.weight: False\n",
      "bert.encoder.layer.10.attention.self.value.bias: False\n",
      "bert.encoder.layer.10.attention.output.dense.weight: False\n",
      "bert.encoder.layer.10.attention.output.dense.bias: False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.10.intermediate.dense.weight: False\n",
      "bert.encoder.layer.10.intermediate.dense.bias: False\n",
      "bert.encoder.layer.10.output.dense.weight: False\n",
      "bert.encoder.layer.10.output.dense.bias: False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.11.attention.self.query.weight: False\n",
      "bert.encoder.layer.11.attention.self.query.bias: False\n",
      "bert.encoder.layer.11.attention.self.key.weight: False\n",
      "bert.encoder.layer.11.attention.self.key.bias: False\n",
      "bert.encoder.layer.11.attention.self.value.weight: False\n",
      "bert.encoder.layer.11.attention.self.value.bias: False\n",
      "bert.encoder.layer.11.attention.output.dense.weight: False\n",
      "bert.encoder.layer.11.attention.output.dense.bias: False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: False\n",
      "bert.encoder.layer.11.intermediate.dense.weight: False\n",
      "bert.encoder.layer.11.intermediate.dense.bias: False\n",
      "bert.encoder.layer.11.output.dense.weight: False\n",
      "bert.encoder.layer.11.output.dense.bias: False\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: False\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: False\n",
      "bert.pooler.dense.weight: False\n",
      "bert.pooler.dense.bias: False\n",
      "classifier.weight: True\n",
      "classifier.bias: True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:15.929264Z",
     "start_time": "2024-12-08T08:46:15.924873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # Замораживаем все параметры\n",
    "\n",
    "# Размораживаем параметры последнего слоя (classifier)\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True  # Размораживаем параметры последнего слоя\n"
   ],
   "id": "994e8b47f04b78a0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:16.169453Z",
     "start_time": "2024-12-08T08:46:16.134919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "f8a751f02f0b5cba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:16.547709Z",
     "start_time": "2024-12-08T08:46:16.303100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#read train csv\n",
    "df = pd.read_csv('datasets/train.csv')\n",
    "X = df['TEXT']\n",
    "Y = df['TOPIC']\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ],
   "id": "6799ac9d316ead6e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:16.678614Z",
     "start_time": "2024-12-08T08:46:16.669564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ],
   "id": "51d8d2cb5b31b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n",
      "(800,)\n",
      "(800,)\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:16.839173Z",
     "start_time": "2024-12-08T08:46:16.817466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "labels = Y.unique()\n",
    "print(labels)\n",
    "#get index of labels in Y\n",
    "Y = Y.apply(lambda x: list(labels).index(x))\n",
    "Y_train = Y_train.apply(lambda x: list(labels).index(x))\n",
    "Y_val = Y_val.apply(lambda x: list(labels).index(x))\n",
    "print(Y_val)"
   ],
   "id": "548c56d255b98367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['политика' 'туризм и путешествия' 'здоровье' 'наука и техника'\n",
      " 'развлечения' 'спорт']\n",
      "521    2\n",
      "737    2\n",
      "740    0\n",
      "660    0\n",
      "411    4\n",
      "      ..\n",
      "408    2\n",
      "332    0\n",
      "208    0\n",
      "613    3\n",
      "78     4\n",
      "Name: TOPIC, Length: 200, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:17.061500Z",
     "start_time": "2024-12-08T08:46:17.049888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_weights = (1000 / Y.value_counts())\n",
    "print(class_weights)"
   ],
   "id": "7d424cdf10f4c70d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC\n",
      "0      2.061856\n",
      "2      3.717472\n",
      "3      7.936508\n",
      "4     13.333333\n",
      "1     28.571429\n",
      "5    100.000000\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:17.223988Z",
     "start_time": "2024-12-08T08:46:17.204592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "# Corresponding labels\n",
    "max_length = 512\n",
    "# dataset = TextClassificationDataset(X, Y, tokenizer, max_length)\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "dataset_train = TextClassificationDataset(X_train.tolist(), Y_train.tolist(), tokenizer, max_length)\n",
    "dataset_test = TextClassificationDataset(X_val.tolist(), Y_val.tolist(), tokenizer, max_length)\n",
    "dataset = TextClassificationDataset(X.tolist(), Y, tokenizer, max_length)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "id": "5cecce272f38ac5e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:17.414388Z",
     "start_time": "2024-12-08T08:46:17.402310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_epochs = 20\n",
    "num_training_steps = len(dataloader_train) * num_epochs\n",
    "\n",
    "# Linear learning rate scheduler with warm-up\n",
    "num_warmup_steps = int(0.1 * num_training_steps)  # 10% of training steps for warm-up\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ],
   "id": "c039ad9517fde60d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:17.634735Z",
     "start_time": "2024-12-08T08:46:17.621618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f1_macro_loss(logits, labels, epsilon=1e-8):\n",
    "    \"\"\"Approximate F1 Macro Loss.\"\"\"\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=logits.size(1)).float()\n",
    "\n",
    "    # Compute True Positives, False Positives, False Negatives\n",
    "    tp = (probs * one_hot_labels).sum(dim=0)\n",
    "    fp = (probs * (1 - one_hot_labels)).sum(dim=0)\n",
    "    fn = ((1 - probs) * one_hot_labels).sum(dim=0)\n",
    "\n",
    "    # Precision and Recall\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    # F1 Score\n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    f1_macro = f1_per_class.mean()\n",
    "\n",
    "    # Loss (maximize F1 -> minimize -F1)\n",
    "    return 1 - f1_macro"
   ],
   "id": "5ff4ded32d473001",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:17.846813Z",
     "start_time": "2024-12-08T08:46:17.834053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(dataloader, name):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:  # Replace with validation dataloader\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"{name} Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "id": "872fb8ce846ca55e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:46:18.057408Z",
     "start_time": "2024-12-08T08:46:18.043581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_macro(dataloader, model):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return f1_score(all_labels, all_preds, average=\"macro\")\n"
   ],
   "id": "7764d23df6153f3c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:50:02.530576359Z",
     "start_time": "2024-12-08T08:49:04.897961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "class_weights = np.sqrt(compute_class_weight('balanced', classes=np.array([0, 1, 2, 3, 4, 5]), y=Y.tolist()))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    calculate_accuracy(dataloader_train, \"Train\")\n",
    "    calculate_accuracy(dataloader_test, \"Test\")\n",
    "    print(calculate_f1_macro(dataloader_train, model))\n",
    "    print(calculate_f1_macro(dataloader_test, model))\n"
   ],
   "id": "e3a86d1ae9f6bf15",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:30<00:00,  1.20s/it, loss=0.00339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.75%\n",
      "Test Accuracy: 100.00%\n",
      "0.9953153989295224\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                                                                                   | 0/25 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 30\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# Update progress bar\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     loop\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m     loop\u001B[38;5;241m.\u001B[39mset_postfix(loss\u001B[38;5;241m=\u001B[39m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     31\u001B[0m calculate_accuracy(dataloader_train, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m calculate_accuracy(dataloader_test, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:48:13.649079814Z",
     "start_time": "2024-12-08T08:42:27.659911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"rubert-classifier\")\n",
    "tokenizer.save_pretrained(\"rubert-classifier\")\n"
   ],
   "id": "4e66abc88286f08b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rubert-classifier/tokenizer_config.json',\n",
       " 'rubert-classifier/special_tokens_map.json',\n",
       " 'rubert-classifier/vocab.txt',\n",
       " 'rubert-classifier/added_tokens.json',\n",
       " 'rubert-classifier/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:48:13.650406719Z",
     "start_time": "2024-12-07T18:08:09.580111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextClassifier:\n",
    "    def __init__(self, model_path=\"rubert-classifier\"):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.labels = ['политика','туризм и путешествия','здоровье','наука и техника',\n",
    "                       'развлечения','спорт']\n",
    "    \n",
    "    def predict(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        return self.labels[predicted_label]\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        result = []\n",
    "        for text in texts:\n",
    "            result.append(self.predict(text))\n",
    "        return result"
   ],
   "id": "56a92776521369eb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:48:13.650756202Z",
     "start_time": "2024-12-07T18:08:10.285540Z"
    }
   },
   "cell_type": "code",
   "source": "classifier = TextClassifier(\"rubert-classifier\")",
   "id": "6d09bfa4d86daa91",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:48:13.650999737Z",
     "start_time": "2024-12-07T18:08:10.634128Z"
    }
   },
   "cell_type": "code",
   "source": "classifier.predict(\"Завтра еду на олимпиаду, хочу пробежать быстрее всех\")",
   "id": "1cab9a022e79e89c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'туризм и путешествия'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9354857bb2b2c3e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
